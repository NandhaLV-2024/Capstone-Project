# -*- coding: utf-8 -*-
"""Capstone

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wQtgr2x-jiHK1OGR1SRV99G7vdCFUjUy
"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, mean_squared_error, precision_score, f1_score, recall_score, confusion_matrix, silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.preprocessing import MinMaxScaler
import time
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.mixture import GaussianMixture

import numpy as np
import pandas as pd
df=pd.read_csv('/content/Project Data.csv')
df.head()

df.describe()

df.info()

df.isnull().sum()

duplicate_rows = df.duplicated()
print(df[duplicate_rows])

df.duplicated().sum()

df.shape

from scipy.stats import zscore
numeric_df = df.select_dtypes(include=[np.number])
z_scores = zscore(numeric_df)
print(z_scores)
abs_z_scores = np.abs(z_scores)
filtered_entries = (abs_z_scores < 3).all(axis=1)
#df_cleaned = df[filtered_entries]
print(filtered_entries)

plt.figure(figsize=(20, 6))
sns.boxplot(df)
plt.show()

# Create a copy of the 'State' column
df['state_copy'] = df['State'].copy()

# Apply LabelEncoder to the parent 'State' column
le = LabelEncoder()
df['State'] = le.fit_transform(df['State'])

if df['state_copy'].dtype == object:
  # Create a LabelEncoder instance
  le = LabelEncoder()
  # Fit the encoder to the 'State' column (learns the categories)
  le.fit(df['state_copy'])

  # Create a DataFrame to store the label information
  state_labels_df = pd.DataFrame()
  state_labels_df['Original_State'] = le.classes_  # List of original state names
  state_labels_df['Encoded_Label'] = range(len(le.classes_))  # Encoded labels (0, 1, 2, ...)

  state_labels_df.to_csv("state_labels.csv", index=False)
  print("State label information saved to state_labels.csv")

else:
  print("'State' column is not a string type. Label encoding is not applicable.")

unique_energy_data = df[['ENERGY_ID', 'ENERGY_NAME']].drop_duplicates()

# Save to CSV file
unique_energy_data.to_csv('energy_data.csv', index=False)

print("CSV file with unique energy data created successfully!")

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
encoded_data = df.apply(label_encoder.fit_transform)

print("Original data:", df)
print("Encoded data:", encoded_data)

encoded_data.head()



encoded_data.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(encoded_data.corr(),annot=True)

encoded_data.head()

import warnings
warnings.filterwarnings("ignore")

import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
distances = []
K_range = range(1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(encoded_data)
    distances.append(kmeans.inertia_)
plt.plot(K_range, distances, marker='o')
plt.xlabel('Number of clusters (K)')
plt.ylabel('Sum of squared distances')
plt.title('Elbow Method for Optimal K')
plt.show()

col = ['ENERGY_ID','CONSUMPTION','UNIT_PRICE','YEAR_TO_DATE','PREVIOUS_YEAR_TO_DATE','SHARE']
from sklearn.preprocessing import StandardScaler
Sc = StandardScaler()
encoded_data[col]= Sc.fit_transform(encoded_data[col])

#Feature Selection

import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler


def PCA_FS(encoded_data):
  pca = PCA()
  pca.fit(encoded_data)

  explained_variance_ratio = pca.explained_variance_ratio_
  cumulative_explained_variance = explained_variance_ratio.cumsum()
  n_components = (cumulative_explained_variance < 0.95).sum() + 1

  pca = PCA(n_components=n_components)
  data_pca = pca.fit_transform(encoded_data)

  encoded_data = pd.DataFrame(data_pca, columns=[f"PC{i}" for i in range(1, n_components + 1)])
  return encoded_data


def corr_Fs(encoded_data):
  cols1 = ['COUNTRY_ID','State','YEAR', 'MONTH', 'ENERGY_ID','ENERGY_NAME', 'CONSUMPTION', 'UNIT_PRICE','YEAR_TO_DATE', 'PREVIOUS_YEAR_TO_DATE','SHARE','PEAK_TIME']
  encoded_data = encoded_data[cols1]
  return encoded_data

def svd_fs(encoded_data):
  ###Singular Value Decomposition (SVD)
  from sklearn.decomposition import TruncatedSVD
  X = encoded_data.values
  svd = TruncatedSVD(n_components=10)
  encoded_data = svd.fit_transform(X)
  return encoded_data

def clustering_fs(encoded_data):
  #Clustering based Feature selection
  from sklearn.cluster import KMeans
  import numpy as np
  X = encoded_data.values
  kmeans = KMeans(n_clusters=4)
  cluster_labels = kmeans.fit_predict(X)

  centroids = kmeans.cluster_centers_
  feature_variance = np.var(centroids, axis=0)

  n_selected_features = 5
  selected_features_indices = np.argsort(feature_variance)[-n_selected_features:]
  selected_features = encoded_data.columns[selected_features_indices]
  return encoded_data[selected_features]

Correlation = corr_Fs(encoded_data)
PCA_data = PCA_FS(encoded_data)
SVD = svd_fs(encoded_data)
Clustering = clustering_fs(encoded_data)

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score


k = 4
silhouette_scores = []
method = ["Correlation","PCA","SVD","Clustering"]
for d in[Correlation,PCA_data,SVD,Clustering]:
    kmeans = KMeans(n_clusters=k)
    cluster_labels = kmeans.fit_predict(d)
    silhouette_avg = silhouette_score(d, cluster_labels)
    silhouette_scores.append(silhouette_avg)

print(silhouette_scores)
for i, score in enumerate(silhouette_scores):
    print(f"Silhouette Score for {method[i]}: {score}")

#MODEL SELECTION
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score


X = PCA_data

clustering_algorithms = [
    ("K-Means", KMeans(n_clusters=4, random_state=42)),
    ("DBSCAN", DBSCAN(eps=0.5, min_samples=5)),
    ("Agglomerative Clustering", AgglomerativeClustering(n_clusters=5)),
    ("Gaussian Mixture Model", GaussianMixture(n_components=5, random_state=42)),
]

silhouette_scores = []
davies_bouldin_indices = []
calinski_harabasz_scores = []

# Fit clustering algorithms to the data and evaluate their performance
for name, algorithm in clustering_algorithms:
    algorithm.fit(X)
    if name != "Gaussian Mixture Model":
        labels = algorithm.labels_
    else:
        labels = algorithm.predict(X)
    silhouette = silhouette_score(X, labels)  if len(set(labels)) > 1 else -1
    db_index = davies_bouldin_score(X, labels) if len(set(labels)) > 1 else -1
    ch_score = calinski_harabasz_score(X, labels) if len(set(labels)) > 1 else -1
    silhouette_scores.append((name, silhouette))
    davies_bouldin_indices.append((name, db_index))
    calinski_harabasz_scores.append((name, ch_score))

print("Evaluation Results:")
print("Silhouette Scores:")
for name, score in silhouette_scores:
    print(f"{name}: {score}")
print()
print("Davies-Bouldin Indices:")
for name, index in davies_bouldin_indices:
    print(f"{name}: {index}")
print()
print("Calinski-Harabasz Scores:")
for name, score in calinski_harabasz_scores:
    print(f"{name}: {score}")

encoded_data.head()
f=pd.DataFrame()
b=['ENERGY_ID','CONSUMPTION','UNIT_PRICE','YEAR_TO_DATE','PREVIOUS_YEAR_TO_DATE','SHARE']
f[b] = encoded_data.loc[:, b]
f.head()

x_copy=f.copy()
y_predict=kmeans.fit_predict(x_copy)
x_copy["Clusters"] = y_predict
cluster_labels=kmeans.labels_
pca = PCA(n_components=2)
x_copy[['PC1', 'PC2']] = pca.fit_transform(x_copy)

x_copy1=f.copy()
x_copy2=f.copy()
x_copy3=f.copy()
gmm = GaussianMixture(n_components=4)
dbscan = DBSCAN(eps=0.2, min_samples=3)
agg_clustering = AgglomerativeClustering(n_clusters=4)
labels = agg_clustering.fit_predict(X)
y_predict=dbscan.fit_predict(x_copy1)
y_predict=agg_clustering.fit_predict(x_copy2)
y_predict=gmm.fit_predict(x_copy3)
x_copy1["Clusters"] = y_predict
x_copy2["Clusters"] = y_predict
x_copy3["Clusters"] = y_predict
cluster_labels=dbscan.labels_
cluster_labels=agg_clustering.labels_
pca = PCA(n_components=2)
x_copy1[['PC1', 'PC2']] = pca.fit_transform(x_copy1)
x_copy2[['PC1', 'PC2']] = pca.fit_transform(x_copy2)
x_copy3[['PC1', 'PC2']] = pca.fit_transform(x_copy3)

#kmeans
plt.figure(figsize=(10, 7))
pl = sns.scatterplot(data = x_copy, x='PC1', y='PC2',hue=x_copy["Clusters"], palette= 'viridis')
plt.legend()
plt.show()

#hierarchical
plt.figure(figsize=(10, 7))
pl = sns.scatterplot(data = x_copy2, x='PC1', y='PC2',hue=x_copy2["Clusters"], palette= 'viridis')
plt.legend()
plt.show()

#gaussian mixture
plt.figure(figsize=(10, 7))
pl = sns.scatterplot(data = x_copy3, x='PC1', y='PC2',hue=x_copy3["Clusters"], palette= 'viridis')
plt.legend()
plt.show()

#dbscan
plt.figure(figsize=(10, 7))
pl = sns.scatterplot(data = x_copy1, x='PC1', y='PC2',hue=x_copy1["Clusters"], palette= 'viridis')
plt.legend()
plt.show()

encoded_data.info()

# Add the cluster number as a new column to the original DataFrame
df['cluster_number'] = kmeans.labels_

# Save the single DataFrame with cluster information
df.to_csv("clustered_data.csv", index=False)

